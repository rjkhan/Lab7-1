---
title: "Lab7"
author: "Andrea Bruzzone, Thomas Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Here we fit a linear regression model and a linear regression model with forward selection of covariates
on the training dataset. 

```{r,message=FALSE,echo=FALSE}
library(Lab7)
library(caret)
library(mlbench)
library(leaps)
data(BostonHousing)
BostonHousing$chas <- as.numeric(BostonHousing$chas)-1

```


```{r, echo=FALSE}
inTrain <- createDataPartition(BostonHousing$crim,
                               p=.75,
                               list=FALSE)

training <- BostonHousing[inTrain,]
testing <- BostonHousing[-inTrain,]

ctrl <- trainControl(
  method = "repeatedcv",
  number = 10)
  

set.seed(4587)
lmfit <- train(crim ~ . ,
               data = training,
               method ="lm"
)
lmfit
```


```{r,echo=FALSE}
set.seed(4587)
lmGrid <-  expand.grid(nvmax=1:(ncol(training)-1))
lmforwardfit <- train(crim ~.,
                      data = training,
                      method ="leapForward",
                      #preProc = c("center","scale"),
                      tuneGrid = lmGrid
)
lmforwardfit

```

We see that the best RMSE value usually comes from the linear regression model with forward selection where there are between two and 12 predictors. This RMSE value is usually a little better than the one from the regular linear regression model.

